"""
Enhanced Query Handler Module - Sequential Post Processing
Processes posts generated by Goal Handler and converts them into next_post_prediction format
with sequential processing and rate limit handling.
"""

import aiohttp
import asyncio
import json
import re
from typing import Dict, List, Any, Optional
from datetime import datetime
from utils.r2_client import R2Client
from utils.status_manager import StatusManager
from utils.logging import logger
from utils.test_filter import TestFilter
from config import GEMINI_CONFIG, R2_CONFIG, STRUCTUREDB_R2_CONFIG
import google.generativeai as genai
from tenacity import retry, stop_after_attempt, wait_exponential
import time

# Configure Gemini
genai.configure(api_key=GEMINI_CONFIG["api_key"])
model = genai.GenerativeModel(
    model_name=GEMINI_CONFIG["model"],
    generation_config={
        "max_output_tokens": GEMINI_CONFIG["max_tokens"],
        "temperature": GEMINI_CONFIG["temperature"],
        "top_p": GEMINI_CONFIG["top_p"],
        "top_k": GEMINI_CONFIG["top_k"]
    }
)

class RateLimiter:
    """Rate limiter with exponential backoff"""
    
    def __init__(self, initial_delay: float = 2.0, max_delay: float = 60.0, backoff_factor: float = 2.0):
        self.current_delay = initial_delay
        self.max_delay = max_delay
        self.backoff_factor = backoff_factor
        self.last_request_time = 0
        self.consecutive_failures = 0
    
    async def wait(self):
        """Wait for the appropriate delay between requests"""
        now = time.time()
        time_since_last = now - self.last_request_time
        
        if time_since_last < self.current_delay:
            wait_time = self.current_delay - time_since_last
            logger.info(f"üïí Rate limiter waiting for {wait_time:.1f}s")
            await asyncio.sleep(wait_time)
        
        self.last_request_time = time.time()
    
    def update_delay(self, success: bool):
        """Update delay based on request success/failure"""
        if success:
            # On success, gradually reduce delay
            self.consecutive_failures = 0
            self.current_delay = max(2.0, self.current_delay / self.backoff_factor)
        else:
            # On failure, increase delay exponentially
            self.consecutive_failures += 1
            self.current_delay = min(
                self.max_delay,
                self.current_delay * self.backoff_factor
            )
            logger.warning(f"‚ö†Ô∏è Rate limit hit, increasing delay to {self.current_delay:.1f}s")

class SequentialQueryHandler:
    """Sequential Query Handler for processing generated_content posts"""
    
    def __init__(self):
        self.r2_client = R2Client(config=R2_CONFIG)  # Tasks bucket
        self.r2_client_structuredb = R2Client(config=STRUCTUREDB_R2_CONFIG)  # Structuredb bucket
        self.platforms = ["instagram", "twitter", "facebook"]  # Support all three platforms
        self.retry_interval = 10  # 10 seconds retry interval
        self.rate_limiter = RateLimiter()  # Initialize rate limiter
        
    async def run_continuous_processing(self):
        """Main processing loop with 10-second retry interval"""
        logger.info("üöÄ Starting Sequential Query Handler with 10-second retry loop")
        
        while True:
            try:
                logger.info("üîç Scanning for pending posts in generated_content...")
                
                # Scan all platforms for pending posts
                processed_any = False
                
                for platform in self.platforms:
                    platform_processed = await self.scan_platform_for_pending_posts(platform)
                    if platform_processed:
                        processed_any = True
                
                if processed_any:
                    logger.info("‚úÖ Processed pending posts, continuing scan...")
                else:
                    logger.info("üí§ No pending posts found, waiting 10 seconds...")
                
                # Wait 10 seconds before next scan
                await asyncio.sleep(self.retry_interval)
                
            except Exception as e:
                logger.error(f"üö® Error in continuous processing loop: {e}")
                await asyncio.sleep(self.retry_interval)
    
    async def scan_platform_for_pending_posts(self, platform: str) -> bool:
        """Scan a specific platform for pending posts and process one at a time"""
        try:
            platform_prefix = f"generated_content/{platform}/"
            objects = await self.r2_client.list_objects(platform_prefix)
            
            # üßπ COMPREHENSIVE TEST FILTERING - Filter out all test objects
            production_objects = TestFilter.filter_test_objects(objects)
            
            # Log filtering statistics
            if len(objects) != len(production_objects):
                filtered_count = len(objects) - len(production_objects)
                logger.info(f"üßπ Query Handler filtered out {filtered_count} test files from {platform}")
            
            # Find posts.json files
            posts_files = []
            for obj in production_objects:
                key = obj["Key"]
                if key.endswith("posts.json"):
                    posts_files.append(key)
            
            logger.debug(f"üìÅ Found {len(posts_files)} production posts.json files for {platform}")
            
            # Process each file to find pending posts
            for posts_file_key in posts_files:
                # Extract username from path: generated_content/platform/username/posts.json
                parts = posts_file_key.split('/')
                if len(parts) >= 4:
                    username = parts[2]
                    
                    # üö´ PRODUCTION FILTER - Additional username check
                    if TestFilter.should_skip_processing(platform, username, posts_file_key):
                        continue  # Skip test users
                    
                    # üéØ PRODUCTION USER - Process posts
                    TestFilter.log_production_user(platform, username, "processing posts")
                    
                    # Check if this file has pending posts
                    if await self.process_file_for_pending_posts(posts_file_key, platform, username):
                        return True  # Processed one post, return to allow retry loop
            
            return False
            
        except Exception as e:
            logger.error(f"üö® Error scanning {platform} for pending posts: {e}")
            return False
    
    async def process_file_for_pending_posts(self, posts_file_key: str, platform: str, username: str) -> bool:
        """Process a specific posts.json file for pending posts"""
        try:
            # Read the posts.json file
            posts_data = await self.r2_client.read_json(posts_file_key)
            
            if not posts_data:
                logger.debug(f"üìÑ Empty or invalid posts file: {posts_file_key}")
                return False
            
            # Find the first pending post
            for post_key, post_value in posts_data.items():
                if post_key.startswith("Post_") and isinstance(post_value, dict):
                    if post_value.get("status") == "pending":
                        logger.info(f"üéØ Found pending post: {post_key} for {username} on {platform}")
                        
                        # Process this specific post
                        success = await self.process_single_post(
                            posts_file_key, 
                            post_key, 
                            post_value, 
                            platform, 
                            username,
                            posts_data
                        )
                        
                        return success  # Return after processing one post
            
            logger.debug(f"‚úÖ No pending posts in {posts_file_key}")
            return False
            
        except Exception as e:
            logger.error(f"üö® Error processing file {posts_file_key}: {e}")
            return False
    
    async def process_single_post(
        self, 
        posts_file_key: str,
        post_key: str, 
        post_value: Dict, 
        platform: str, 
        username: str,
        full_posts_data: Dict
    ) -> bool:
        """Process a single pending post and transform it to next_post_prediction format"""
        try:
            logger.info(f"üîÑ Processing {post_key} for {username} on {platform}")
            
            # Load profile data for context
            profile_data = await self.load_profile_data(username, platform)
            
            # Transform the post content using AI
            transformed_post = await self.transform_post_content(
                post_value.get("content", ""),
                platform,
                username,
                profile_data
            )
            
            if not transformed_post:
                logger.error(f"‚ùå Failed to transform {post_key}")
                return False
            
            # Create next_post_prediction format
            next_post_data = {
                "module_type": "next_post_prediction",
                "platform": platform,
                "username": username,
                "post_data": transformed_post,
                "generated_at": datetime.now().isoformat()
            }
            
            # Save to next_posts directory
            output_success = await self.save_next_post(next_post_data, platform, username)
            
            if output_success:
                # Mark original post as processed
                full_posts_data[post_key]["status"] = "processed"
                update_success = await self.r2_client.write_json(posts_file_key, full_posts_data)
                
                if update_success:
                    logger.info(f"‚úÖ Successfully processed {post_key} for {username} on {platform}")
                    return True
                else:
                    logger.error(f"‚ùå Failed to update status for {post_key}")
            else:
                logger.error(f"‚ùå Failed to save transformed post for {post_key}")
            
            return False
            
        except Exception as e:
            logger.error(f"üö® Error processing single post {post_key}: {e}")
            return False
    
    async def load_profile_data(self, username: str, platform: str) -> Optional[Dict]:
        """Load profile data for context in transformation"""
        try:
            profile_key = f"{platform}/{username}/{username}.json"
            profile_data = await self.r2_client_structuredb.read_json(profile_key)
            
            if isinstance(profile_data, list) and len(profile_data) > 0:
                profile_data = profile_data[0]
            
            return profile_data
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Could not load profile data for {username} on {platform}: {e}")
            return None
    
    async def transform_post_content(
        self, 
        content: str, 
        platform: str, 
        username: str, 
        profile_data: Optional[Dict]
    ) -> Optional[Dict]:
        """Transform 3-sentence content into next_post_prediction format using AI with rate limit handling"""
        
        # Create transformation prompt
        prompt = self.create_transformation_prompt(content, platform, username, profile_data)
        
        max_retries = 5
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                # Wait for rate limiter before making request
                await self.rate_limiter.wait()
                
                # Generate transformation using Gemini
                response = await asyncio.wait_for(
                    model.generate_content_async(prompt),
                    timeout=60
                )
                
                # Update rate limiter on success
                self.rate_limiter.update_delay(success=True)
                
                # Parse the response
                transformed_data = self.parse_transformation_response(response.text)
                
                if transformed_data:
                    return transformed_data
                
            except Exception as e:
                retry_count += 1
                error_str = str(e)
                
                # Check if it's a rate limit error
                if "429" in error_str or "quota" in error_str.lower():
                    # Update rate limiter on failure
                    self.rate_limiter.update_delay(success=False)
                    
                    if retry_count < max_retries:
                        logger.warning(f"‚ö†Ô∏è Rate limit hit, attempt {retry_count}/{max_retries}. Waiting before retry...")
                        continue
                
                logger.error(f"üö® Error in AI transformation: {e}")
                if retry_count < max_retries:
                    logger.info(f"Retrying... Attempt {retry_count}/{max_retries}")
                    continue
                
                break
        
        # If we've exhausted retries or hit a non-recoverable error, use fallback
        logger.warning(f"‚ö†Ô∏è Failed to transform after {retry_count} attempts, using fallback")
        return self.create_fallback_transformation(content, platform, username)
    
    def create_transformation_prompt(
        self, 
        content: str, 
        platform: str, 
        username: str, 
        profile_data: Optional[Dict]
    ) -> str:
        """Create detailed prompt for content transformation"""
        
        # Extract profile insights if available
        bio = ""
        follower_info = ""
        if profile_data:
            bio = profile_data.get("biography", "")
            followers = profile_data.get("followersCount", 0)
            follower_info = f"Followers: {followers:,}"
        
        # üè∑Ô∏è HASHTAG PRESERVATION: Extract existing hashtags from content
        existing_hashtags = self._extract_hashtags_from_content(content)
        hashtag_instruction = ""
        if existing_hashtags:
            hashtag_instruction = f"\nIMPORTANT: The content already includes optimized hashtags: {' '.join(existing_hashtags)}. Please include these hashtags in your response."
        
        prompt = f"""
        Transform this 3-sentence content into a engaging {platform} post for {username}.
        
        ORIGINAL CONTENT: {content}
        
        ACCOUNT INFO:
        - Platform: {platform}
        - Username: {username}
        - Bio: {bio}
        - {follower_info}
        {hashtag_instruction}
        
        TRANSFORMATION REQUIREMENTS:
        1. Create an engaging caption that matches the platform style
        2. Extract and enhance key hashtags (5-8 hashtags total, including any existing ones)
        3. Create a compelling call-to-action
        4. Generate detailed image prompt from the visual description
        
        For {platform}:
        {"- Use casual, engaging tone with emojis" if platform == "instagram" else "- Use conversational, community-focused tone" if platform == "facebook" else "- Use concise, impactful language"}
        {"- Focus on visual storytelling" if platform == "instagram" else "- Focus on community engagement and conversations" if platform == "facebook" else "- Focus on quick engagement"}
        {"- Include relevant hashtags in caption" if platform == "instagram" else "- Use community-focused hashtags" if platform == "facebook" else "- Keep hashtags minimal"}
        
        RESPOND ONLY with this JSON format:
        {{
            "caption": "Engaging post caption with personality and platform optimization",
            "hashtags": ["#hashtag1", "#hashtag2", "#hashtag3", "#hashtag4", "#hashtag5"],
            "call_to_action": "Compelling call-to-action that encourages engagement",
            "image_prompt": "Detailed description for image generation with visual direction and styling"
        }}
        """
        
        return prompt
    
    def _extract_hashtags_from_content(self, content: str) -> List[str]:
        """üè∑Ô∏è Extract existing hashtags from enhanced content"""
        if not content:
            return []
        
        # Find all hashtags in the content
        hashtags = re.findall(r'#\w+', content)
        logger.debug(f"üè∑Ô∏è Extracted {len(hashtags)} existing hashtags from content")
        return hashtags
    
    def parse_transformation_response(self, response_text: str) -> Optional[Dict]:
        """Parse AI response into structured format"""
        try:
            # Extract JSON from response
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response_text[json_start:json_end]
                data = json.loads(json_str)
                
                # Validate required fields
                required_fields = ["caption", "hashtags", "call_to_action", "image_prompt"]
                if all(field in data for field in required_fields):
                    return data
                else:
                    logger.error(f"‚ùå Missing required fields in AI response")
                    
        except Exception as e:
            logger.error(f"üö® Error parsing AI response: {e}")
            
        return None
    
    def create_fallback_transformation(self, content: str, platform: str, username: str) -> Dict:
        """Create fallback transformation when AI fails"""
        
        # Extract sentences
        sentences = content.split('. ') if content else []
        
        # Create basic caption
        if len(sentences) >= 2:
            caption = '. '.join(sentences[:2])
        else:
            caption = content if content else "Exciting content coming your way!"
        
        # Basic hashtags based on platform
        if platform == "instagram":
            hashtags = ["#Instagram", "#Content", "#Engagement", "#Quality", "#Brand"]
        elif platform == "facebook":
            hashtags = ["#Facebook", "#Community", "#SocialConnection", "#Engagement", "#Content"]
        else:  # Twitter
            hashtags = ["#Twitter", "#Update", "#Engagement"]
        
        # Extract image prompt from 3rd sentence or create default
        image_prompt = "High-quality, engaging visual content"
        if len(sentences) >= 3:
            third_sentence = sentences[2]
            if any(word in third_sentence.lower() for word in ["visual", "image", "photo", "look", "show"]):
                image_prompt = third_sentence
        
        return {
            "caption": caption,
            "hashtags": hashtags,
            "call_to_action": "What do you think? Share your thoughts!",
            "image_prompt": image_prompt
        }
    
    async def save_next_post(self, next_post_data: Dict, platform: str, username: str) -> bool:
        """Save transformed post to next_posts directory with campaign naming convention"""
        try:
            # Create output directory path
            output_dir = f"next_posts/{platform}/{username}/"
            
            # List existing files to determine next number
            try:
                objects = await self.r2_client.list_objects(output_dir)
                
                # üè∑Ô∏è FIX 5: Always use campaign_next_post_*.json naming convention
                existing_posts = [
                    obj["Key"] for obj in objects 
                    if "campaign_next_post_" in obj["Key"] and obj["Key"].endswith(".json")
                ]
                post_number = len(existing_posts) + 1
                logger.info(f"üìä Current campaign post number: {post_number} for {username}")
            except:
                post_number = 1
                logger.info(f"üìä Starting with campaign post number: 1 for {username}")
            
            # üîß FIX 5: Create output filename with campaign naming convention
            output_key = f"{output_dir}campaign_next_post_{post_number}.json"
            
            # Save the transformed post
            success = await self.r2_client.write_json(output_key, next_post_data)
            
            if success:
                logger.info(f"üíæ Saved transformed campaign post to: {output_key}")
                return True
            else:
                logger.error(f"‚ùå Failed to save to: {output_key}")
                return False
                
        except Exception as e:
            logger.error(f"üö® Error saving campaign next_post: {e}")
            return False

# Main execution
async def main():
    """Main entry point for sequential query handler"""
    handler = SequentialQueryHandler()
    await handler.run_continuous_processing()

if __name__ == "__main__":
    asyncio.run(main())