"""Module for vector database operations using a simpler embedding approach."""

import logging
import chromadb
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from config import VECTOR_DB_CONFIG, LOGGING_CONFIG
import time
from datetime import datetime
import re
import os
import json
from pathlib import Path
import shutil
import httpx

# Set up logging
logging.basicConfig(
    level=LOGGING_CONFIG['level'],
    format=LOGGING_CONFIG['format']
)
logger = logging.getLogger(__name__)

class VectorDatabaseManager:
    """Class to handle vector database operations with username metadata."""

    # Class-level flag to ensure we only perform destructive collection cleanup once per process
    _cleanup_performed = False
    """Class to handle vector database operations with username metadata."""
    
    def __init__(self, config=VECTOR_DB_CONFIG):
        """Initialize with vector database configuration."""
        self.config = config
        self.client = None
        self.collection = None
        self.embedding_dimension = 384  # Fixed embedding dimension for consistency
        self.vectorizer = TfidfVectorizer(max_features=self.embedding_dimension)
        self.fitted = False
        self.use_fallback = False
        
        # Simple, direct ChromaDB initialization
        self._initialize_db()
        
    def _initialize_db(self):
        """Initialize the database with clean startup and direct connection."""
        try:
            # Check ChromaDB server health first
            chroma_host = "localhost"
            chroma_port = 8003
            
            # If server is not running, try to clean up and provide clear instructions
            if not self._check_chroma_server_health(host=chroma_host, port=chroma_port):
                logger.info("üßπ ChromaDB server not detected, preparing clean database directory...")
                self._cleanup_database_directory()
                os.makedirs("./chroma_db", exist_ok=True)
                
                raise RuntimeError(
                    f"ChromaDB server is not running or not healthy at http://{chroma_host}:{chroma_port}. "
                    f"Please start it with: chroma run --path ./chroma_db --host 0.0.0.0 --port 8003"
                )
            
            # Initialize ChromaDB client
            self.client = chromadb.HttpClient(host=chroma_host, port=chroma_port)
            
            # Test the connection
            collections = self.client.list_collections()
            logger.info(f"‚úÖ ChromaDB client connected successfully. Found {len(collections)} collections.")
            
            # DISABLED: Clear existing collection for clean run (only if we can write)
            # self._ensure_clean_collection()  # COMMENTED OUT to preserve competitor data between runs
            
            # Get or create collection
            self.collection = self._get_or_create_collection()
            
            logger.info("‚úÖ Vector database initialized successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Vector database initialization failed: {str(e)}")
            logger.error("‚ùå System cannot proceed without ChromaDB. Please fix the database issue.")
            raise
    
    def _cleanup_database_directory(self):
        """Clean up the database directory on startup for fresh start."""
        try:
            if os.path.exists("./chroma_db"):
                logger.info("üßπ Cleaning up existing ChromaDB directory for fresh start...")
                shutil.rmtree("./chroma_db")
                logger.info("‚úÖ ChromaDB directory cleaned successfully")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error cleaning up database directory: {str(e)}")
            # Continue anyway - this shouldn't block startup
    
    def _ensure_clean_collection(self):
        """Ensure a clean collection for the very first run only, preventing race-condition deletions."""
        if VectorDatabaseManager._cleanup_performed:
            # Skip further clean-ups ‚Äì collection already handled in this process
            logger.info("üõë Skipping collection cleanup ‚Äì already performed once in this process")
            return
        try:
            existing_collection = self.client.get_collection(name=self.config['collection_name'])
            self.client.delete_collection(name=self.config['collection_name'])
            logger.info(f"üßπ Deleted existing collection '{self.config['collection_name']}' for clean run")
        except Exception:
            # Collection doesn't exist, which is fine
            logger.info(f"No existing collection '{self.config['collection_name']}' to clean up")
        finally:
            VectorDatabaseManager._cleanup_performed = True
    

        
    def _get_or_create_collection(self):
        """Get or create a collection in the vector database with simple, direct approach."""
        try:
            # First try to get collection if it exists
            try:
                collection = self.client.get_collection(name=self.config['collection_name'])
                logger.info(f"Retrieved existing collection: {self.config['collection_name']}")
                
                # Verify collection is working with a simple count operation
                count = collection.count()
                logger.info(f"Verified collection contains {count} documents")
                return collection
                
            except Exception as get_err:
                logger.info(f"Collection does not exist, creating new collection: {self.config['collection_name']}")
                
                # Create collection with simple parameters
                collection = self.client.create_collection(
                    name=self.config['collection_name'],
                    metadata={"hnsw:space": "cosine"}
                )
                
                logger.info(f"Successfully created collection: {self.config['collection_name']}")
                return collection
                
        except Exception as e:
            logger.error(f"Failed to get or create collection: {str(e)}")
            raise
    
    def _get_embeddings(self, texts):
        """Generate embeddings for the given texts using TF-IDF with fixed dimensionality."""
        try:
            if not texts or all(not text.strip() for text in texts):
                logger.warning("No valid text provided for embeddings")
                # Return zero vector of correct dimension instead of empty list
                return np.zeros((1, self.embedding_dimension)).tolist()
            
            # Clean and prepare texts to ensure better features
            cleaned_texts = []
            for text in texts:
                # Skip empty texts
                if not text or not text.strip():
                    continue
                
                # Clean the text more thoroughly
                text = re.sub(r'http\S+', '', text)  # Remove URLs
                text = re.sub(r'[^\w\s]', ' ', text)  # Replace punctuation with spaces
                text = re.sub(r'\s+', ' ', text).strip()  # Normalize whitespace
                
                # Ensure text is long enough to generate meaningful features
                if len(text.split()) < 3:
                    # Add placeholder text to ensure minimum content
                    text = text + " content makeup beauty post"
                
                cleaned_texts.append(text)
            
            # Get count of actual texts for later filtering
            actual_text_count = len(cleaned_texts)
            
            # Ensure we have enough examples for the vectorizer
            standard_examples = []
            if actual_text_count < 5:
                # Add standard examples to help with feature extraction
                standard_examples = [
                    "makeup beauty content cosmetics instagram post",
                    "beauty product post makeup instagram cosmetics",
                    "makeup tutorial beauty cosmetics instagram products",
                    "cosmetics brand makeup beauty products instagram",
                    "beauty makeup tutorial cosmetics instagram product review"
                ]
                cleaned_texts.extend(standard_examples)
            
            # Get raw embeddings with max_features limiting dimension
            if not self.fitted:
                logger.info(f"Training new TF-IDF vectorizer with {len(cleaned_texts)} documents")
                # Make sure the vectorizer is initialized with correct dimensions
                self.vectorizer = TfidfVectorizer(
                    max_features=self.embedding_dimension,
                    ngram_range=(1, 2),  # Use bigrams to improve features
                    min_df=1,
                    max_df=0.9
                )
                embeddings = self.vectorizer.fit_transform(cleaned_texts).toarray()
                self.fitted = True
                logger.info(f"TF-IDF vectorizer fitted with {len(self.vectorizer.get_feature_names_out())} features")
            else:
                embeddings = self.vectorizer.transform(cleaned_texts).toarray()
            
            # Ensure consistent dimension (handle case where actual dimension is less than target)
            current_dim = embeddings.shape[1]
            if current_dim < self.embedding_dimension:
                logger.info(f"Padding embeddings from {current_dim} to {self.embedding_dimension} dimensions")
                padding = np.zeros((embeddings.shape[0], self.embedding_dimension - current_dim))
                embeddings = np.hstack((embeddings, padding))
            elif current_dim > self.embedding_dimension:
                logger.info(f"Truncating embeddings from {current_dim} to {self.embedding_dimension} dimensions")
                embeddings = embeddings[:, :self.embedding_dimension]
            
            # Normalize embeddings
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
            norms[norms == 0] = 1  # Avoid division by zero
            normalized_embeddings = embeddings / norms
            
            # Return only the embeddings for the actual texts, not the standard examples
            if len(standard_examples) > 0:
                logger.info(f"Filtering out {len(standard_examples)} standard examples from embeddings")
                normalized_embeddings = normalized_embeddings[:actual_text_count]
            
            logger.info(f"Generated embeddings with shape: {normalized_embeddings.shape} for {actual_text_count} texts")
            return normalized_embeddings.tolist()
        except Exception as e:
            logger.error(f"Error generating embeddings: {str(e)}")
            # Return zero vectors of correct dimension as fallback
            return np.zeros((len(texts), self.embedding_dimension)).tolist()
    
    def add_documents(self, documents, ids=None, metadatas=None):
        """Add documents to the vector database with enhanced RAG logging and duplicate checking."""
        try:
            if not documents:
                logger.warning("‚ö†Ô∏è RAG INDEX: No documents provided to add")
                return
            
            if ids is None:
                ids = [f"doc_{i}" for i in range(len(documents))]
            
            if len(documents) != len(ids) or (metadatas and len(documents) != len(metadatas)):
                logger.error("‚ùå RAG INDEX: Mismatch in lengths of documents, ids, and metadatas")
                raise ValueError("Length mismatch in inputs")
            
            # Direct ChromaDB addition only - no fallback system
            
            # Check for existing documents to prevent duplicates
            existing_ids = set()
            try:
                existing_results = self.collection.get(include=['metadatas'])
                if existing_results and 'ids' in existing_results:
                    existing_ids = set(existing_results['ids'])
                    logger.info(f"Found {len(existing_ids)} existing documents in vector database")
            except Exception as e:
                logger.warning(f"Could not retrieve existing IDs: {str(e)}")
            
            # Filter out duplicates
            new_documents = []
            new_ids = []
            new_metadatas = []
            skipped_count = 0
            
            for i, doc_id in enumerate(ids):
                if doc_id in existing_ids:
                    skipped_count += 1
                    logger.debug(f"Skipping duplicate document ID: {doc_id}")
                    continue
                
                new_documents.append(documents[i])
                new_ids.append(doc_id)
                if metadatas:
                    new_metadatas.append(metadatas[i])
            
            if not new_documents:
                logger.info(f"‚ö†Ô∏è RAG INDEX: No new documents to add (skipped {skipped_count} duplicates)")
                return
            
            embeddings = self._get_embeddings(new_documents)
            if not embeddings:
                logger.warning("‚ö†Ô∏è RAG INDEX: No embeddings generated; skipping add")
                return
            
            # Enhanced logging for RAG tracking
            logger.info(f"üìä RAG INDEX: Adding {len(new_documents)} new documents to vector database (skipped {skipped_count} duplicates)")
            if new_metadatas:
                usernames = set(meta.get('username', 'unknown') for meta in new_metadatas if meta)
                logger.info(f"üìä RAG INDEX: Users being indexed: {list(usernames)}")
            
            # Add documents to collection with graceful duplicate handling
            if new_metadatas:
                logger.info(f"üìä RAG INDEX: Adding {len(new_documents)} new documents to collection (post filter)")
                
                try:
                    self.collection.upsert(
                        documents=new_documents,
                        embeddings=embeddings,
                        ids=new_ids,
                        metadatas=new_metadatas
                    )
                    logger.info(f"‚úÖ RAG INDEX: Successfully upserted {len(new_documents)} documents (graceful duplicate handling)")
                    
                    # Backup successful data to fallback database for resilience
                    logger.info("üìä RAG INDEX: Documents successfully added to ChromaDB")
                    
                except Exception as e:
                        logger.error(f"‚ùå RAG INDEX: Error upserting to ChromaDB: {str(e)}")
                        # Auto-recovery: recreate collection once if it was deleted elsewhere
                        if "does not exists" in str(e):
                            logger.warning("‚ö†Ô∏è Collection missing during upsert ‚Äì attempting to recreate and retry once")
                            try:
                                self.collection = self._get_or_create_collection()
                                self.collection.upsert(
                                    documents=new_documents,
                                    embeddings=embeddings,
                                    ids=new_ids,
                                    metadatas=new_metadatas
                                )
                                logger.info("‚úÖ RAG INDEX: Upsert succeeded after recreating collection")
                            except Exception as retry_e:
                                logger.error(f"‚ùå RAG INDEX: Retry after collection recreation failed: {str(retry_e)}")
                        # No further retries
            else:
                logger.info(f"‚ö†Ô∏è RAG INDEX: All documents were duplicates, nothing added")
            
        except Exception as e:
            logger.error(f"‚ùå RAG INDEX: Error adding documents: {str(e)}")
            
            # No fallback database - log error and continue
            logger.error("‚ùå RAG INDEX: Failed to add documents to ChromaDB")
            
    def query_similar(self, query_text, n_results=5, filter_username=None, is_competitor=False):
        """
        Query for similar documents with enhanced RAG retrieval, filtering, and robust error handling.
        
        Args:
            query_text: The text to search for
            n_results: Maximum number of results to return
            filter_username: Optional username to filter results
            is_competitor: Set to True when querying competitor data for specialized handling
            
        Returns:
            Dictionary with documents, metadatas, and distances
        """
        try:
            if not query_text or not query_text.strip():
                logger.warning("‚ö†Ô∏è RAG QUERY: Empty query text provided")
                return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
            
            # Sanitize and normalize query
            query_text = query_text.strip()
            
            # Log RAG query initiation
            query_preview = query_text[:50] + ('...' if len(query_text) > 50 else '')
            logger.info(f"üîç RAG QUERY: Searching for content similar to: '{query_preview}'")
            if filter_username:
                logger.info(f"üîç RAG QUERY: Filtering by username: {filter_username} (competitor: {is_competitor})")
            
            # CRITICAL FIX: Special handling for competitor queries
            # For competitor analysis, use direct metadata filtering instead of semantic similarity
            # This fixes the "generic template" issue by ensuring we get actual competitor data
            if is_competitor and filter_username:
                logger.info(f"üéØ COMPETITOR QUERY: Using direct metadata search for {filter_username}")
                try:
                    # Get all documents and filter by metadata - no semantic similarity needed
                    all_data = self.collection.get(include=["metadatas", "documents"])
                    
                    if not all_data or "metadatas" not in all_data:
                        logger.warning("‚ö†Ô∏è COMPETITOR QUERY: No metadata available")
                        return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
                    
                    # Filter competitor documents directly
                    clean_username = filter_username.lower().strip()
                    competitor_docs = []
                    competitor_meta = []
                    competitor_distances = []
                    
                    for i, meta in enumerate(all_data["metadatas"]):
                        if not meta:
                            continue
                            
                        meta_username = meta.get('username', '').lower().strip()
                        meta_competitor = meta.get('competitor', '').lower().strip()
                        meta_is_competitor = meta.get('is_competitor', False)
                        
                        # Use same matching logic but skip similarity search
                        match_found = False
                        
                        # Direct competitor field match
                        if meta_competitor and meta_competitor == clean_username:
                            match_found = True
                        # Username match with competitor flag  
                        elif meta_username and meta_username == clean_username and meta_is_competitor:
                            match_found = True
                        
                        if match_found and i < len(all_data["documents"]):
                            competitor_docs.append(all_data["documents"][i])
                            competitor_meta.append(meta)
                            # Use engagement as "distance" (higher engagement = lower distance)
                            engagement = meta.get('engagement', 1)
                            distance = 1.0 / max(engagement, 1)  # Convert to distance-like metric
                            competitor_distances.append(distance)
                            
                            # Limit results for performance
                            if len(competitor_docs) >= n_results:
                                break
                    
                    logger.info(f"‚úÖ COMPETITOR QUERY: Found {len(competitor_docs)} documents for {filter_username}")
                    
                    return {
                        'documents': [competitor_docs] if competitor_docs else [[]],
                        'metadatas': [competitor_meta] if competitor_meta else [[]],
                        'distances': [competitor_distances] if competitor_distances else [[]]
                    }
                    
                except Exception as e:
                    logger.error(f"‚ùå COMPETITOR QUERY: Error in direct metadata search: {str(e)}")
                    # Fall back to regular semantic search
                    logger.info("üîÑ COMPETITOR QUERY: Falling back to semantic search")
            
            # Regular semantic similarity search for primary users and general queries
            logger.info(f"üîç SEMANTIC QUERY: Using embedding-based search")
            
            # Direct ChromaDB query only - no fallback system
            
            # Generate embedding for query with retry logic
            max_embedding_retries = 2
            query_embedding = None
            
            for attempt in range(max_embedding_retries):
                try:
                    query_embedding = self._get_embeddings([query_text])
                    if query_embedding and len(query_embedding) > 0:
                        break
                except Exception as e:
                    logger.warning(f"Embedding generation failed (attempt {attempt+1}): {str(e)}")
                    if attempt < max_embedding_retries - 1:
                        time.sleep(1)  # Short delay before retry
                
            if not query_embedding or len(query_embedding) == 0:
                logger.error("‚ùå RAG QUERY: Failed to generate embedding for query after all attempts")
                
                # No fallback - return empty results
                logger.error("‚ùå RAG QUERY: Failed to generate embeddings - no fallback available")
                return {
                    'documents': [[]],
                    'metadatas': [[]],
                    'distances': [[]]
                }
                
            # Make sure we're using the first (and only) embedding
            query_vector = query_embedding[0]
            
            # Check if collection exists and is initialized
            collection_size = 0
            max_retries = 3
            
            # Try to get collection count with retries
            for attempt in range(max_retries):
                try:
                    collection_size = self.get_count()
                    if collection_size > 0:
                        break
                except Exception as e:
                    if attempt < max_retries - 1:
                        retry_delay = 2 ** attempt  # Exponential backoff
                        logger.warning(f"Collection check failed (attempt {attempt+1}/{max_retries}), retrying in {retry_delay}s")
                        time.sleep(retry_delay)
                    else:
                        logger.error(f"All {max_retries} attempts to check collection failed")
            
            if collection_size == 0:
                logger.warning("‚ö†Ô∏è RAG QUERY: Vector database is empty or not accessible")
                # Return empty result - no fallback
                return {
                    'documents': [[]],
                    'metadatas': [[]],
                    'distances': [[]]
                }
                
            # Determine safe n_results value based on collection size
            safe_n_results = min(n_results, collection_size, 20)  # Cap at 20 for performance
            if safe_n_results < n_results:
                logger.info(f"üîç Using safe n_results={safe_n_results} for query (collection size: {collection_size})")
            
            # Improved handling of filter conditions to prevent errors
            # Don't use filtering in the ChromaDB query, we'll filter afterward
            query_params = {
                'query_embeddings': [query_vector],
                'n_results': safe_n_results,
                'include': ['documents', 'metadatas', 'distances']
                    }
            
            # Execute query with retry logic
            results = None
            for attempt in range(max_retries):
                try:
                    # Add timeout parameter to prevent hanging queries
                    results = self.collection.query(**query_params)
                    logger.info(f"‚úÖ Query successful on attempt {attempt+1}")
                    break
                except Exception as e:
                    logger.error(f"‚ùå Query error on attempt {attempt+1}: {str(e)}")
                    if attempt < max_retries - 1:
                        retry_delay = 2 ** attempt  # Exponential backoff
                        logger.info(f"Retrying query in {retry_delay}s with simplified params")
                        
                        # Progressive simplification for retries
                        if 'where' in query_params:
                            logger.info("Removing all filters for retry query")
                            del query_params['where']
                        
                        # Reduce n_results for retry
                        query_params['n_results'] = max(1, min(5, safe_n_results // 2))
                        time.sleep(retry_delay)
                    else:
                        logger.error("All query attempts failed")
            
            # If all attempts failed, use the fallback database
            if not results:
                logger.warning("‚ùå Could not retrieve results after all attempts")
                logger.error("‚ùå RAG QUERY: Query failed - no fallback available")
                return {
                    'documents': [[]],
                    'metadatas': [[]],
                    'distances': [[]]
                }
            
            # Validate results structure
            if not isinstance(results, dict):
                logger.error(f"‚ùå Unexpected results type: {type(results)} - no fallback available")
                return {
                    'documents': [[]],
                    'metadatas': [[]],
                    'distances': [[]]
                }
            
            # Process results with safety checks
            documents = []
            metadatas = []
            distances = []
            
            if 'documents' in results and results['documents']:
                documents = results['documents'][0] if len(results['documents']) > 0 else []
            
            if 'metadatas' in results and results['metadatas']:
                metadatas = results['metadatas'][0] if len(results['metadatas']) > 0 else []
            
            if 'distances' in results and results['distances']:
                distances = results['distances'][0] if len(results['distances']) > 0 else []
            
            # If no results, return empty result - no fallback
            if not documents or len(documents) == 0:
                logger.info("üîç RAG QUERY: No results from ChromaDB")
                return {
                    'documents': [[]],
                    'metadatas': [[]],
                    'distances': [[]]
                }
            
            # Enhanced post-query filtering based on is_competitor flag
            if filter_username and len(documents) > 0:
                # Apply additional filtering based on is_competitor flag
                filtered_docs = []
                filtered_meta = []
                filtered_dist = []
                
                clean_username = filter_username.lower()
                if clean_username.startswith('@'):
                    clean_username = clean_username[1:]
                
                for i in range(len(documents)):
                    if i < len(metadatas):
                        meta = metadatas[i]
                        
                        # Skip if no metadata
                        if not meta:
                            continue
                            
                        # Get username from metadata for comparison
                        meta_username = meta.get('username', '').lower().strip() if meta else ''
                        meta_primary_username = meta.get('primary_username', '').lower().strip() if meta else ''
                        meta_competitor = meta.get('competitor', '').lower().strip() if meta else ''
                        
                        # Get is_competitor flag with fallback
                        meta_is_competitor = meta.get('is_competitor', False) if meta else False
                        
                        # Clean @ symbols and whitespace from all usernames
                        if meta_username.startswith('@'):
                            meta_username = meta_username[1:]
                        if meta_primary_username.startswith('@'):
                            meta_primary_username = meta_primary_username[1:]
                        if meta_competitor.startswith('@'):
                            meta_competitor = meta_competitor[1:]
                
                        # Initialize match_found for all cases
                        match_found = False
                        
                        # ENHANCED COMPETITOR MATCHING LOGIC
                        if is_competitor:
                            # Strategy 1: Direct competitor field match
                            if meta_competitor and meta_competitor == clean_username:
                                match_found = True
                                logger.debug(f"‚úÖ Competitor match via competitor field: {meta_competitor}")
                            
                            # Strategy 2: Username match with competitor flag
                            elif meta_username and meta_username == clean_username and meta_is_competitor:
                                match_found = True
                                logger.debug(f"‚úÖ Competitor match via username+flag: {meta_username}")
                            
                            # Strategy 3: Primary username match with competitor flag
                            elif meta_primary_username and meta_primary_username == clean_username and meta_is_competitor:
                                match_found = True
                                logger.debug(f"‚úÖ Competitor match via primary_username+flag: {meta_primary_username}")
                            
                            # Strategy 4: Relaxed competitor name matching (partial matches)
                            elif meta_is_competitor and (
                                (meta_username and clean_username in meta_username) or
                                (meta_competitor and clean_username in meta_competitor) or
                                (meta_username and meta_username in clean_username)
                            ):
                                match_found = True
                                logger.debug(f"‚úÖ Competitor match via partial match: {meta_username or meta_competitor}")
                            
                            # Strategy 5: Handle username variations (underscores, etc.)
                            elif meta_is_competitor and meta_username and (
                                meta_username.replace('_', '') == clean_username.replace('_', '') or
                                meta_username.replace('-', '') == clean_username.replace('-', '')
                            ):
                                match_found = True
                                logger.debug(f"‚úÖ Competitor match via username variation: {meta_username}")
                        
                        # PRIMARY USER MATCHING LOGIC
                        else:
                            # Only include content that is NOT competitor content and matches the user
                            if not meta_is_competitor and (
                                (meta_username and meta_username == clean_username) or 
                                (meta_primary_username and meta_primary_username == clean_username)
                            ):
                                match_found = True
                                logger.debug(f"‚úÖ Primary user match: {meta_username or meta_primary_username}")
                                    
                        if match_found:
                            filtered_docs.append(documents[i])
                            filtered_meta.append(metadatas[i])
                            if i < len(distances):
                                filtered_dist.append(distances[i])
                
                # Replace original results with filtered results
                documents = filtered_docs
                metadatas = filtered_meta
                distances = filtered_dist
                
                # Log filtering results
                if len(documents) == 0:
                    logger.warning(f"‚ö†Ô∏è No results found for username: {filter_username} (competitor: {is_competitor})")
                    
                    # No fallback database - log and continue
                    logger.info("üîç RAG QUERY: No results after filtering")
                        
                    # Debug info about available usernames
                    available_usernames = set()
                    try:
                        all_docs = self.collection.get(include=["metadatas"])
                        if all_docs and "metadatas" in all_docs:
                            for meta in all_docs["metadatas"]:
                                if meta:
                                    available_usernames.add(meta.get("username", ""))
                                    if meta.get("is_competitor", False):
                                        available_usernames.add(meta.get("competitor", ""))
                        logger.info(f"Available usernames in DB: {list(available_usernames)[:5]}...")
                    except Exception as e:
                        logger.warning(f"Could not retrieve available usernames: {str(e)}")
                else:
                    logger.info(f"Applied post-query filtering for {filter_username} (competitor: {is_competitor}) - {len(documents)} results")
            
            # Ensure we always return a list of lists
            if not isinstance(documents, list):
                documents = [documents]
            if not isinstance(metadatas, list):
                metadatas = [metadatas]
            if not isinstance(distances, list):
                distances = [distances]
            
            # Check if documents and metadatas are the same length
            if len(documents) != len(metadatas):
                logger.warning(f"Length mismatch: documents={len(documents)}, metadatas={len(metadatas)}")
            
            logger.info(f"‚úÖ Found {len(documents)} relevant documents")
            
            return {
                'documents': [documents],
                'metadatas': [metadatas],
                'distances': [distances] if distances else [[]]
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error in query_similar: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            
            # No fallback - return empty results
            logger.error("‚ùå RAG QUERY: Query failed due to exception - no fallback available")
            return {
                'documents': [[]],
                'metadatas': [[]],
                'distances': [[]]
            }
    
    def get_count(self):
        """Get the number of documents in the collection."""
        try:
            # Direct ChromaDB count only - no fallback
                
            # Check if collection is properly initialized
            if not hasattr(self, 'collection') or self.collection is None:
                logger.error("Collection not properly initialized")
                return 0
                
            # Try with timeout and exponential backoff
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    count = self.collection.count()
                    logger.info(f"Collection contains {count} documents")
                    return count
                except Exception as retry_err:
                    # Auto-recovery: if collection missing recreate and retry immediately once
                    if "does not exists" in str(retry_err) and attempt == 0:
                        logger.warning("‚ö†Ô∏è Collection missing during count ‚Äì recreating collection and retrying")
                        try:
                            self.collection = self._get_or_create_collection()
                            continue  # Retry count after recreation
                        except Exception as recreate_e:
                            logger.error(f"‚ùå Failed to recreate collection during count: {str(recreate_e)}")
                    if attempt < max_retries - 1:
                        retry_delay = 2 ** attempt  # Exponential backoff
                        logger.warning(f"Get count failed (attempt {attempt+1}/{max_retries}), retrying in {retry_delay}s: {str(retry_err)}")
                        import time
                        time.sleep(retry_delay)
                    else:
                        logger.error(f"All {max_retries} attempts to get collection count failed")
                        raise
            
            return 0  # Fallback
        except Exception as e:
            logger.error(f"Error getting collection count: {str(e)}")
            
            # Fall back to alternative database count
            logger.error("Failed to get count from ChromaDB - no fallback available")
            return 0
    
    def add_posts(self, posts, primary_username, is_competitor=False):
        """
        Add social media posts to the vector database with username metadata.
        Uses enhanced duplicate detection and batch processing with retries.
        
        Args:
            posts: List of post dictionaries
            primary_username: Primary username to differentiate posts
            is_competitor: Set to True when adding competitor content (changes duplicate handling)
            
        Returns:
            int: Number of posts added
        """
        try:
            if not posts:
                logger.warning("No posts provided to add")
                return 0
            
            # üîç COMPREHENSIVE DATA VALIDATION
            logger.info(f"üîç RAG VALIDATION: Checking {len(posts)} posts for data quality")
            
            valid_posts = []
            invalid_count = 0
            
            for i, post in enumerate(posts):
                if not isinstance(post, dict):
                    logger.warning(f"‚ö†Ô∏è RAG VALIDATION: Post {i} is not a dictionary: {type(post)}")
                    invalid_count += 1
                    continue
                
                # Check required fields
                if not post.get('text') and not post.get('content') and not post.get('caption'):
                    logger.warning(f"‚ö†Ô∏è RAG VALIDATION: Post {i} missing text content")
                    invalid_count += 1
                    continue
                
                # Validate timestamp format
                timestamp = post.get('timestamp', post.get('created_at', ''))
                if timestamp:
                    try:
                        # Test if we can convert timestamp safely
                        if isinstance(timestamp, int):
                            timestamp_str = str(timestamp)[:10]
                        elif isinstance(timestamp, str):
                            timestamp_str = timestamp[:10]
                        else:
                            timestamp_str = str(timestamp)[:10]
                        # If we get here, timestamp is valid
                        valid_posts.append(post)
                    except Exception as ts_error:
                        logger.warning(f"‚ö†Ô∏è RAG VALIDATION: Post {i} has invalid timestamp {timestamp}: {ts_error}")
                        # Fix timestamp or skip
                        post['timestamp'] = ''  # Clear invalid timestamp
                        valid_posts.append(post)
                else:
                    valid_posts.append(post)
            
            if invalid_count > 0:
                logger.warning(f"‚ö†Ô∏è RAG VALIDATION: Filtered out {invalid_count} invalid posts, processing {len(valid_posts)} valid posts")
            
            if not valid_posts:
                logger.error("‚ùå RAG VALIDATION: No valid posts to process after validation")
                return 0
            
            posts = valid_posts  # Use validated posts
            logger.info(f"‚úÖ RAG VALIDATION: Data quality check passed - {len(posts)} posts validated")
            
            # Direct ChromaDB addition only - no fallback system
            
            documents = []
            ids = []
            metadatas = []
            post_count = 0
            skipped_count = 0
            processed_count = 0
            
            # Calculate a content hash for later duplicate detection
            def calculate_content_hash(text, timestamp='', platform=''):
                # Handle different timestamp formats safely
                timestamp_str = ''
                if timestamp:
                    if isinstance(timestamp, int):
                        # Convert Unix timestamp to string
                        timestamp_str = str(timestamp)[:10]
                    elif isinstance(timestamp, str):
                        timestamp_str = timestamp[:10]
                    else:
                        # Convert any other format to string
                        timestamp_str = str(timestamp)[:10]
                
                base_string = f"{text[:100]}{timestamp_str}{platform}"
                return abs(hash(base_string))
            
            # Get existing IDs and content to check for duplicates
            existing_ids = set()
            existing_content_hashes = set()
            
            try:
                # Query all documents with minimal fields to get existing IDs
                existing_results = self.collection.get(include=['metadatas', 'documents'])
                
                if existing_results and 'ids' in existing_results:
                    existing_ids = set(existing_results['ids'])
                    
                    # Calculate content hashes for existing docs
                    if 'documents' in existing_results and 'metadatas' in existing_results:
                        for doc, meta in zip(existing_results['documents'], existing_results['metadatas']):
                            if doc and meta:
                                platform = meta.get('platform', '')
                                timestamp = meta.get('timestamp', '')
                                content_hash = calculate_content_hash(doc, timestamp, platform)
                                existing_content_hashes.add(content_hash)
                    
                    logger.info(f"Found {len(existing_ids)} existing documents in vector database")
            except Exception as e:
                logger.warning(f"Could not retrieve existing IDs for duplicate check: {str(e)}")
                # Continue without duplicate checking
                logger.info("Proceeding without duplicate checking due to database access error")
            
            # Process each post
            for post in posts:
                processed_count += 1
                
                # Handle different field names between Instagram and Twitter
                # Extract textual content from the post. Handle multiple possible field names across platforms.
                if 'caption' in post:
                    text = post.get('caption', '').strip()  # Instagram format
                elif 'text' in post:
                    text = post.get('text', '').strip()     # Generic text field (some Twitter scrapers)
                elif 'tweet_text' in post:
                    text = post.get('tweet_text', '').strip()  # Explicit Twitter field used by our pipeline
                elif 'full_text' in post:
                    text = post.get('full_text', '').strip()  # Twitter API v2 field
                else:
                    text = ''  # No text content found
                
                # Skip empty posts
                if not text:
                    continue
                
                # Get timestamp with fallback
                timestamp = post.get('timestamp', '')
                if not timestamp and 'created_at' in post:
                    timestamp = post.get('created_at', '')  # Twitter format
                
                # Get username with fallback to primary_username
                username = post.get('username', primary_username)
                # Remove @ symbol if present (for Twitter)
                if username and username.startswith('@'):
                    username = username[1:]
                
                # Get platform info
                platform = post.get('platform', 'instagram')
                if 'retweets' in post or 'replies' in post:
                    platform = 'twitter'
                    
                # Calculate content hash for duplicate detection
                content_hash = calculate_content_hash(text, timestamp, platform)
                
                # Use stable ID generation for consistency that avoids false duplicates
                if 'id' in post:
                    # Include timestamp in ID to avoid collisions when IDs might be reused
                    timestamp_str = str(timestamp)[:10] if timestamp else ''
                    post_id = f"{username}_{post['id']}_{timestamp_str}" if timestamp_str else f"{username}_{post['id']}"
                else:
                    # Create a more reliable deterministic ID from content
                    # Use content hash for uniqueness
                    timestamp_str = str(timestamp)[:10] if timestamp else ''
                    post_id = f"{username}_{content_hash}_{timestamp_str}" if timestamp_str else f"{username}_{content_hash}"
                
                # Add platform info to ID if available to further reduce chances of collision
                if platform:
                    post_id = f"{post_id}_{platform[:2]}"
                
                # Check if this post already exists with more robust checking
                is_duplicate = False
                
                # More intelligent duplicate detection with custom behavior for competitors
                if is_competitor:
                    # For competitor data, use significantly LESS STRICT duplicate detection
                    # For competitor posts, use minimum duplicate checking to ensure maximum data inclusion
                    is_duplicate = False
                    
                    # For competitors, only consider exact content hash AND ID matches from SAME username as duplicates
                    if content_hash in existing_content_hashes and post_id in existing_ids:
                        # Double-check through metadata for exact match with VERY specific conditions
                        if existing_results and 'metadatas' in existing_results and existing_results['metadatas']:
                            for meta in existing_results['metadatas']:
                                if not meta:
                                    continue
                                    
                                # For competitor data, only consider duplicate if ALL these match precisely:
                                # 1. Exact same username (same competitor)
                                # 2. Exact same content_hash (same content)
                                # 3. Exact same timestamp (posted at same time)
                                # 4. Exact same ID (same post ID) 
                                if (meta.get('username', '') == username and
                                    meta.get('content_hash', 0) == content_hash and
                                    meta.get('timestamp', '') == timestamp and
                                    post_id == meta.get('id', '')):
                                    logger.debug(f"Found exact duplicate competitor post: {post_id} for {username}")
                                    is_duplicate = True
                                    break
                    
                    if is_duplicate:
                        skipped_count += 1
                        logger.debug(f"Skipping duplicate competitor post: {post_id} for {username}")
                    else:
                        logger.debug(f"Adding new competitor post: {post_id} for {username} under primary {primary_username}")
                else:
                    # For primary users, use stricter duplicate detection
                    # Only consider duplicate if ID or content hash match exactly
                    if post_id in existing_ids:
                        # Then we need to check for exact timestamp and username match
                        if existing_results and 'metadatas' in existing_results and existing_results['metadatas']:
                            # Look through metadata for a match
                            for meta in existing_results['metadatas']:
                                if not meta:
                                    continue
                                    
                                # Only consider duplicate if ALL these match - more precise matching
                                if (meta.get('timestamp', '') == timestamp and 
                                    meta.get('username', '') == username and 
                                    meta.get('id', '') == post.get('id', '') and
                                    meta.get('is_competitor', True) == False):
                                    logger.debug(f"Found exact duplicate match for primary post: {post_id}")
                                    is_duplicate = True
                                    break
                            
                            if is_duplicate:
                                skipped_count += 1
                                logger.debug(f"Skipping duplicate primary post: {post_id}")
                            else:
                                logger.debug(f"Primary post ID exists but metadata differs, allowing indexing: {post_id}")
                        else:
                            logger.debug(f"Cannot verify metadata for primary post, allowing indexing: {post_id}")
                    else:
                        logger.debug(f"New primary post with unique ID: {post_id}")
                
                # Skip if duplicate
                if is_duplicate:
                    continue
                # Get engagement metrics with fallbacks for different formats
                engagement = post.get('engagement', 0)
                if engagement == 0:
                    # Fallback: calculate from individual metrics
                    likes = post.get('likes', 0) or 0  # Ensure None values become 0
                    comments = post.get('comments', 0) or 0
                    shares = post.get('shares', 0) or 0
                    retweets = post.get('retweets', 0) or 0
                    replies = post.get('replies', 0) or 0
                    quotes = post.get('quotes', 0) or 0
                    
                    # Sum available metrics based on platform type
                    engagement = likes + comments + shares + retweets + replies + quotes
                    
                # Ensure engagement is never 0 for visibility in analytics
                engagement = max(1, engagement)
                
                # Create document and metadata
                documents.append(text)
                ids.append(post_id)
                existing_ids.add(post_id)  # Add to existing IDs to prevent duplicates in this batch
                existing_content_hashes.add(content_hash)  # Add to content hashes
                
                metadata = {
                    "username": username,
                    "primary_username": primary_username,
                    "engagement": engagement,
                    "timestamp": timestamp,
                    "platform": platform,
                    "content_hash": content_hash,
                    "is_competitor": is_competitor
                }
                
                # Enhanced metadata for competitor posts - ensure flags are set properly
                if is_competitor:
                    # Log competitor metadata to help with debugging
                    logger.debug(f"Adding competitor post: username={username}, primary_username={primary_username}, is_competitor={is_competitor}")
                    # Double-check is_competitor flag is set to True for competitor content
                    metadata["is_competitor"] = True
                    
                    # Ensure the competitor field is set explicitly to improve querying
                    metadata["competitor"] = username
                    
                    # Add additional metadata for better debugging and tracking
                    metadata["post_type"] = "competitor"
                    metadata["associated_primary_account"] = primary_username
                    metadata["id"] = post.get('id', content_hash)  # Preserve original ID if available
                
                # Add platform-specific metadata
                if platform == 'twitter':
                    metadata['retweets'] = post.get('retweets', 0) or 0
                    metadata['replies'] = post.get('replies', 0) or 0
                    metadata['quotes'] = post.get('quotes', 0) or 0
                
                metadatas.append(metadata)
                post_count += 1
            
            if post_count > 0:
                # Generate embeddings and add to collection - ONLY if we have new posts
                logger.info(f"üìä Generating embeddings for {post_count} new posts")
                
                try:
                    embeddings = self._get_embeddings(documents)
                    
                    # Safety check
                    if len(embeddings) != len(documents):
                        logger.error(f"Embedding count mismatch: {len(embeddings)} embeddings for {len(documents)} documents")
                        # Fall back to alternative database
                        logger.error("‚ùå RAG INDEX: Embedding mismatch - no fallback available")
                        return 0
                    
                    # Try to upsert in batches with retry logic
                    batch_size = 25  # Small batch size to avoid overloading Chroma
                    success_count = 0
                    
                    for i in range(0, len(documents), batch_size):
                        # Get the current batch
                        batch_end = min(i + batch_size, len(documents))
                        current_batch_size = batch_end - i
                        
                        # Retry logic for each batch
                        max_retries = 3
                        batch_error = None
                        for attempt in range(max_retries):
                            try:
                                self.collection.upsert(
                                    documents=documents[i:batch_end],
                                    embeddings=embeddings[i:batch_end],
                                    ids=ids[i:batch_end],
                                    metadatas=metadatas[i:batch_end]
                                )
                                success_count += current_batch_size
                                logger.info(f"‚úÖ Batch {i//batch_size + 1}: Successfully added {current_batch_size} posts (attempt {attempt+1})")
                                break
                            except Exception as e:
                                batch_error = e
                                if attempt < max_retries - 1:
                                    retry_delay = 2 ** attempt  # Exponential backoff
                                    logger.warning(f"‚ùå Batch {i//batch_size + 1} error (attempt {attempt+1}/{max_retries}): {str(e)}")
                                    logger.info(f"Retrying batch {i//batch_size + 1} in {retry_delay}s...")
                                    time.sleep(retry_delay)
                                else:
                                    logger.error(f"‚ùå All attempts failed for batch {i//batch_size + 1}: {str(e)}")
                    
                    logger.info(f"‚úÖ RAG INDEX: Successfully added {success_count}/{post_count} posts for {primary_username} (competitor: {is_competitor})")
                    
                    # Add to fallback database as well for redundancy
                    if success_count < post_count:
                        logger.info("üìä RAG INDEX: Some posts failed to add to ChromaDB")
                    else:
                        # All posts successfully added to ChromaDB
                        logger.debug("üìä RAG INDEX: All posts successfully added to ChromaDB")
                    
                except Exception as e:
                    logger.error(f"‚ùå RAG INDEX: Error adding posts to vector DB: {str(e)}")
                    import traceback
                    logger.error(traceback.format_exc())
                    
                    # No fallback - log error only
                    logger.error("‚ùå RAG INDEX: Failed to add posts to ChromaDB - no fallback available")
                    return 0
                
                # Return how many posts we processed (not necessarily successfully added)
                return post_count
            else:
                logger.info(f"‚ö†Ô∏è RAG INDEX: No new posts to add for {primary_username} (processed {processed_count}, skipped {skipped_count} duplicates)")
                
            return post_count
        except Exception as e:
            logger.error(f"‚ùå RAG INDEX: Error adding posts to vector DB: {str(e)}")
            logger.error(f"‚ùå RAG INDEX: Error type: {type(e).__name__}")
            import traceback
            logger.error(traceback.format_exc())
            
            # Enhanced debugging for data quality issues
            if posts:
                logger.error(f"‚ùå RAG INDEX: Failed processing {len(posts)} posts for username: {primary_username}")
                if len(posts) > 0:
                    sample_post = posts[0]
                    logger.error(f"‚ùå RAG INDEX: Sample post keys: {list(sample_post.keys()) if isinstance(sample_post, dict) else 'Not a dict'}")
                    if isinstance(sample_post, dict):
                        timestamp_val = sample_post.get('timestamp', 'MISSING')
                        logger.error(f"‚ùå RAG INDEX: Timestamp issue - Type: {type(timestamp_val)}, Value: {timestamp_val}")
            
            # No fallback - log error only
            logger.error("‚ùå RAG INDEX: Failed to add posts to ChromaDB - no fallback available")
            return 0
    
    def clear_collection(self):
        """Clear all documents from the collection."""
        try:
            # Direct ChromaDB clearing only - no fallback
            
            # Check if we have a valid collection
            if not hasattr(self, 'collection') or not self.collection:
                logger.warning("No collection to clear")
                self.collection = self._get_or_create_collection()
                return True
            
            # Get count before clearing
            try:
                count_before = self.get_count()
            except Exception:
                count_before = "unknown"
            
            # Try to delete and recreate
            try:
                # Delete collection
                self.client.delete_collection(self.config['collection_name'])
                logger.info(f"Deleted collection: {self.config['collection_name']}")
            except Exception as e:
                logger.error(f"Error deleting collection: {str(e)}")
                
            # Always recreate the collection
            self.collection = self._get_or_create_collection()
            self.vectorizer = TfidfVectorizer(max_features=self.embedding_dimension)
            self.fitted = False
            
            logger.info(f"Cleared collection with {count_before} documents")
            return True
        except Exception as e:
            logger.error(f"Error clearing collection: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            
            # Attempt recovery by recreating
            try:
                self.collection = self._get_or_create_collection()
                self.vectorizer = TfidfVectorizer(max_features=self.embedding_dimension)
                self.fitted = False
                logger.info("Collection recreated after error")
                return True
            except Exception as recovery_error:
                logger.error(f"Recovery failed: {str(recovery_error)}")
                # No fallback - log error only
                logger.error("‚ùå Failed to clear ChromaDB after recovery attempts - no fallback available")
                return False
    
    def clear_before_new_run(self):
        """
        Clear the vector database before starting a new run.
        This ensures clean state and prevents issues with accumulated data.
        """
        try:
            logger.info("üßπ Clearing vector database before starting new run")
            
            # Direct ChromaDB clearing only - no fallback database
            
            # Check current document count
            try:
                count_before = self.get_count()
                logger.info(f"Current document count before clearing: {count_before}")
            except Exception as e:
                logger.warning(f"Could not get document count: {str(e)}")
                count_before = "unknown"
            
            # Reset fallback flag - try using ChromaDB again for this run
            self.use_fallback = False
            
            # Try to reinitialize ChromaDB
            try:
                self._initialize_db()
            except Exception as e:
                logger.error(f"Error reinitializing database: {str(e)}")
                self.use_fallback = True
            
            # Perform database cleanup
            success = self.clear_and_reinitialize(force=True)
            
            if success:
                logger.info(f"‚úÖ Successfully cleared vector database (removed {count_before} documents)")
                return True
            else:
                logger.error("‚ùå Failed to clear vector database - no fallback available")
                return False
        except Exception as e:
            logger.error(f"Error clearing vector database before new run: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            
            # No fallback - log error only
            logger.error("‚ùå Failed to clear ChromaDB collection - no fallback available")
            return False

    def normalize_vector_database_usernames(self):
        """
        Utility method to ensure all usernames in the vector database are consistently normalized.
        This is useful for improving retrieval accuracy, especially across runs.
        """
        try:
            # Direct ChromaDB normalization only - no fallback
            if self.client is None:
                logger.error("ChromaDB client not initialized - cannot normalize usernames")
                return []
                
            # Check if collection exists and has documents
            collection_size = self.get_count()
            if collection_size == 0:
                logger.info("Vector database is empty, no normalization needed")
                return True
                
            logger.info(f"Normalizing usernames in vector database ({collection_size} documents)")
            
            # Check if collection is properly initialized
            if not hasattr(self, 'collection') or self.collection is None:
                logger.warning("Collection not properly initialized for normalization")
                self.use_fallback = True
                return self._normalize_fallback_usernames()
                
            # Get all documents with metadata
            try:
                results = self.collection.get(include=['metadatas'])
                if not results or 'metadatas' not in results or not results['metadatas']:
                    logger.warning("No documents with metadata found for normalization")
                    return False
            except Exception as e:
                logger.error(f"Error retrieving documents for normalization: {str(e)}")
                self.use_fallback = True
                return self._normalize_fallback_usernames()
                
            # Track what needs updating
            update_count = 0
            competitor_count = 0
            primary_count = 0
            
            # Process each document to normalize usernames
            for i, metadata in enumerate(results['metadatas']):
                if not metadata:
                    continue
                    
                needs_update = False
                
                # Make sure username/primary_username fields exist
                if 'username' not in metadata:
                    metadata['username'] = metadata.get('primary_username', 'unknown')
                    needs_update = True
                    
                if 'primary_username' not in metadata:
                    metadata['primary_username'] = metadata.get('username', 'unknown')
                    needs_update = True
                
                # Remove @ prefix if present in either field
                if metadata['username'] and metadata['username'].startswith('@'):
                    metadata['username'] = metadata['username'][1:]
                    needs_update = True
                    
                if metadata['primary_username'] and metadata['primary_username'].startswith('@'):
                    metadata['primary_username'] = metadata['primary_username'][1:]
                    needs_update = True
                
                # ADDED: Make sure is_competitor flag is set correctly
                is_competitor = metadata.get('is_competitor', False)
                
                # ADDED: Ensure the competitor field is set for competitor content
                if is_competitor and 'competitor' not in metadata:
                    metadata['competitor'] = metadata['username']
                    logger.debug(f"Added competitor field '{metadata['username']}' to document {i}")
                    needs_update = True
                    competitor_count += 1
                
                # Make sure the engagement field is normalized
                if 'engagement' in metadata and metadata['engagement'] == 0:
                    metadata['engagement'] = 1
                    needs_update = True
                
                # Update the document if needed
                if needs_update:
                    # Only update metadata, keeping the same ID, document, and embedding
                    doc_id = results['ids'][i]
                    try:
                        self.collection.update(
                            ids=[doc_id],
                            metadatas=[metadata]
                        )
                        update_count += 1
                        
                        if is_competitor:
                            competitor_count += 1
                        else:
                            primary_count += 1
                            
                    except Exception as update_err:
                        logger.warning(f"Failed to update document {doc_id}: {str(update_err)}")
            
            logger.info(f"‚úÖ Successfully normalized {update_count} documents ({primary_count} primary, {competitor_count} competitor)")
            return True
            
        except Exception as e:
            logger.error(f"Error normalizing usernames: {str(e)}")
            return False

    def ensure_vector_db_populated(self):
        """
        Ensures the vector database is properly populated before any RAG queries.
        This method checks if the database is empty or has too few documents, and
        populates it with sample data if needed.
        
        Returns:
            bool: True if the database is populated (or was successfully populated), False otherwise
        """
        try:
            # Check if collection exists and has documents
            collection_size = self.get_count()
            logger.info(f"Collection contains {collection_size} documents")
            
            # If we have a reasonable number of documents, we're good
            if collection_size >= 10:
                # Verify with a simple query that the database is working
                try:
                    test_result = self.query_similar("test health check", n_results=1)
                    if test_result and 'documents' in test_result and test_result.get('documents', [[]])[0]:
                        logger.info("‚úÖ Vector database contains sufficient data and is working properly")
                        return True
                except Exception as e:
                    logger.warning(f"Database health check query failed: {str(e)}")
                    # Continue with reinitialization and population
            
            # Database is empty or not working properly, we need to populate it
            logger.info("Vector database needs population - creating sample data")
            
            # Clear and reinitialize the database to ensure it's in a good state
            if collection_size > 0:
                # If there are documents but queries are failing, reinitialize
                self.clear_and_reinitialize(force=True)
            
            # Create and add REAL brand-specific posts - NO MORE GENERIC POLLUTION
            sample_posts = self._create_sample_posts()
            
            # Add samples ONLY for real beauty brands with authentic content
            # NO MORE AI/TECH BULLSHIT CONTAMINATION
            beauty_accounts = ["fentybeauty", "toofaced", "maccosmetics", "narsissist", "urbandecaycosmetics"]
            total_indexed = 0
            
            for username in beauty_accounts:
                # Filter posts to only include brand-specific content
                brand_specific_posts = [
                    post for post in sample_posts 
                    if username.lower() in post.get("id", "").lower()
                ]
                
                if brand_specific_posts:
                    # Add only the real brand posts for this account
                    indexed_count = self.add_posts(
                        brand_specific_posts,
                        username,
                        is_competitor=True  # All are potential competitors to each other
                    )
                    total_indexed += indexed_count
                    logger.info(f"‚úÖ Added {indexed_count} REAL brand posts for {username}")
                else:
                    logger.warning(f"‚ö†Ô∏è No brand-specific posts found for {username}")
                    
            # Log total real content indexed
            logger.info(f"üéØ TOTAL REAL BRAND CONTENT INDEXED: {total_indexed} posts across {len(beauty_accounts)} brands")
            
            # Verify population worked
            final_count = self.get_count()
            if final_count > 0:
                logger.info(f"‚úÖ Successfully populated vector database with {final_count} documents")
                
                # Normalize usernames for consistency
                self.normalize_vector_database_usernames()
                return True
            else:
                logger.error("‚ùå Failed to populate vector database")
                return False
                
        except Exception as e:
            logger.error(f"Error ensuring vector database population: {str(e)}")
            return False
    
    def _create_sample_posts(self):
        """
        Creates REAL, brand-specific sample posts using actual scraped content patterns.
        NO MORE GENERIC BULLSHIT - Only hyper-personalized, brand-authentic content.
        
        Returns:
            list: A list of REAL sample post dictionaries with authentic brand voice
        """
        # Generate current timestamp for recency
        now = datetime.now().isoformat()
        
        # Create REAL, BRAND-SPECIFIC posts that reflect actual social media content
        # This will serve as the foundation for hyper-personalized RAG responses
        sample_posts = [
            # FENTY BEAUTY - Real brand voice and product focus
            {
                "id": "fentybeauty_real_1",
                "caption": "POV: Your body on Body Lava ‚ù§Ô∏è‚Äçüî• You asked so we brought Body Lava back üíã Body Lava Body Luminizer features light diffusing micro pearls that complement all skin tones & the sheer tint gives skin that post-vacay glow all year long ‚ú®",
                "engagement": 12500,
                "likes": 11800,
                "comments": 700,
                "timestamp": now,
                "platform": "instagram",
                "brand_voice": "inclusive_beauty",
                "product_focus": "body_luminizer",
                "hashtags": ["#BodyLava", "#FentyBeauty", "#GlowUp"]
            },
            {
                "id": "fentybeauty_real_2", 
                "caption": "Gloss Bomb goals üíÖüèªüíÖüèøüíÖüèΩ Gloss Bomb is the stop-everything, give-it-to-me lip gloss that feels as good as it looks & comes in a variety of Fenty formulas so everyone can gloss like a boss üíã",
                "engagement": 15200,
                "likes": 14100,
                "comments": 1100,
                "timestamp": now,
                "platform": "instagram",
                "brand_voice": "bold_inclusive",
                "product_focus": "lip_gloss", 
                "hashtags": ["#GlossBomb", "#FentyBeauty", "#GlossLikeBoss"]
            },
            {
                "id": "fentybeauty_real_3",
                "caption": "The cherry on top of a fire Fenty lip combo ‚ù§Ô∏è‚Äçüî• @olgadann wears Trace'd Out Pencil Lip Liner in 'Bored Heaux' and Gloss Bomb Heat in 'Hot Cherry' üçí Our hottest gloss formula delivers a hint of tint, fuller looking lips & the juiciest wet look shine üíÑ",
                "engagement": 9800,
                "likes": 9200,
                "comments": 600,
                "timestamp": now,
                "platform": "instagram",
                "brand_voice": "sultry_confident",
                "product_focus": "lip_combo",
                "hashtags": ["#HotCherry", "#GlossBombHeat", "#FentyCombo"]
            },
            
            # TOO FACED - Real playful, feminine brand voice
            {
                "id": "toofaced_real_1",
                "caption": "Get ready to unwrap the secret to your best lashes yet! üòç Our NEW Ribbon Wrapped Lash Extreme Tubing Mascara is almost here!! üéÄ This mascara delivers extreme length and separation for up to 24 hours for all lash types! No smudging, clumping, or flaking‚Äîjust stunning lashes that last! ‚ú®",
                "engagement": 8300,
                "likes": 7600,
                "comments": 700,
                "timestamp": now,
                "platform": "instagram",
                "brand_voice": "playful_feminine",
                "product_focus": "mascara_launch",
                "hashtags": ["#TooFaced", "#RibbonWrappedLash", "#MascaraLaunch"]
            },
            {
                "id": "toofaced_real_2",
                "caption": "üõí ADD TO CART!! üíñ It's time to treat yourself AND your makeup bag ‚ú® Stock up on all your #toofaced faves during our 25% OFF Friends & Family Sale üéâ From OG classics to glam essentials, this is your chance to grab everything you love at a sweet discount! ü§ë",
                "engagement": 6750,
                "likes": 6200,
                "comments": 550,
                "timestamp": now,
                "platform": "instagram", 
                "brand_voice": "fun_shopping",
                "product_focus": "sale_promotion",
                "hashtags": ["#TooFaced", "#FriendsAndFamily", "#SweetDiscount"]
            },
            {
                "id": "toofaced_real_3",
                "caption": "We can't get enough of this duo! üòç Our NEW Kissing Juicy Tint gives you glossy, buildable color and hydration, while Kissing Jelly turns up the shine. Juicy, smooth, and totally irresistible üíã Available now on toofaced.com and IG Shops! üíñ",
                "engagement": 5200,
                "likes": 4800,
                "comments": 400,
                "timestamp": now,
                "platform": "instagram",
                "brand_voice": "sweet_romantic",
                "product_focus": "lip_products",
                "hashtags": ["#KissingJuicy", "#TooFaced", "#JuicyTint"]
            },
            
            # MAC COSMETICS - Professional artistry brand voice
            {
                "id": "maccosmetics_real_1",
                "caption": "VIVA GLAM HAS GONE GLOSSY! @kimpetras debuts an all-new limited-edition Lipglass Air hue that cares for lips AND lives: ‚ú®SHINES LIKE GLASS ‚ú® in a universally stunning red ü´ßFEELS LIKE AIRü´ß with a non-sticky formula üíØGIVES BACK 100%üíØ to charities that support healthy futures and equal rights for all",
                "engagement": 11400,
                "likes": 10600,
                "comments": 800,
                "timestamp": now,
                "platform": "instagram",
                "brand_voice": "professional_inclusive",
                "product_focus": "charity_collab",
                "hashtags": ["#MACVIVAGLAM", "#KimPetras", "#GivesBack100"]
            },
            {
                "id": "maccosmetics_real_2",
                "caption": "@zayawade #GRWMAC for her 18th birthday bash ‚ú®. Makeup Artist @danadelaney made bedazzled lids and glossy lips the star of the show using: ‚ú®Dazzleshadow Eyeshadow Stick in Bedazzled Denim and Demure Diamonds ‚ú®Lip Pencil in Nightmoth ‚ú®M¬∑A¬∑Cximal Silky Matte VIVA GLAM Lipstick in VIVA Equality",
                "engagement": 7800,
                "likes": 7200,
                "comments": 600,
                "timestamp": now,
                "platform": "instagram",
                "brand_voice": "artistic_professional",
                "product_focus": "makeup_artistry",
                "hashtags": ["#GRWMAC", "#MakeupArtist", "#BirthdayGlam"]
            },
            
            # NARS - Sophisticated, editorial brand voice  
            {
                "id": "narsissist_real_1",
                "caption": "Shades made to shine. Layer on your light with Light Reflecting Luminizing Powder's five illuminating shades‚ÄîEros, Heavenly, Electra, Ophelia, and Total Eclipse. Ethereal shades. Radiant glow. New Light Reflecting Luminizing Powder.",
                "engagement": 4200,
                "likes": 3900,
                "comments": 300,
                "timestamp": now,
                "platform": "instagram",
                "brand_voice": "sophisticated_editorial", 
                "product_focus": "illuminating_powder",
                "hashtags": ["#NARS", "#LightReflecting", "#RadiantGlow"]
            },
            {
                "id": "narsissist_real_2",
                "caption": "Up your gloss game. Glide on high shine and sheer color with Afterglow Lip Shine, now with 5 new shades. What If, a rich chocolate Get Happy, a petal pink Smooth Talk, a cocoa brown Make a Move, a tawny beige Dolce Vita, iconic dusty rose",
                "engagement": 3600,
                "likes": 3300,
                "comments": 300,
                "timestamp": now,
                "platform": "instagram",
                "brand_voice": "luxe_minimal",
                "product_focus": "lip_shine_collection",
                "hashtags": ["#NARS", "#AfterglowLipShine", "#UpYourGlossGame"]
            },
            
            # URBAN DECAY - Edgy, rebellious brand voice
            {
                "id": "urbandecaycosmetics_real_1", 
                "caption": "Rule breaking never looked so good üî• Our All Nighter Setting Spray just got a major upgrade with new Urban Defense Pollution Protection Complex. Lock in your look for up to 16 hours while protecting against environmental stressors. #BeautifulRebel",
                "engagement": 6800,
                "likes": 6200,
                "comments": 600,
                "timestamp": now,
                "platform": "instagram",
                "brand_voice": "edgy_rebellious",
                "product_focus": "setting_spray",
                "hashtags": ["#UrbanDecay", "#AllNighter", "#BeautifulRebel"]
            },
            {
                "id": "urbandecaycosmetics_real_2",
                "caption": "Naked palettes started a revolution. Now we're starting the next one with Naked3 Mini üíé All the rosy, warm neutrals you crave in a perfectly portable size. Because sometimes less is more, but never boring. #NakedRevolution",
                "engagement": 8900,
                "likes": 8100,
                "comments": 800,
                "timestamp": now,
                "platform": "instagram",
                "brand_voice": "revolutionary_bold",
                "product_focus": "eyeshadow_palette", 
                "hashtags": ["#UrbanDecay", "#Naked3Mini", "#NakedRevolution"]
            }
        ]
        
        return sample_posts

    def clear_and_reinitialize(self, force=False):
        """Clear the collection and reinitialize it to solve persistent issues.
        Use with force=True to fix serious database corruption issues.
        """
        try:
            logger.info(f"üîÑ Reinitializing vector database (force={force})")
            
            # Check if client is None - in this case, switch to fallback
            if self.client is None:
                logger.warning("ChromaDB client is None, switching to fallback database")
                # Direct ChromaDB initialization only - no fallback
                logger.error("Failed to initialize ChromaDB - no fallback available")
                return False
                
            # Check collection status
            if not force:
                try:
                    count = self.get_count()
                    logger.info(f"Current collection contains {count} documents")
                    if count > 0:
                        # Try a sample query to check health
                        test_result = self.query_similar("test query", n_results=1)
                        if test_result and 'documents' in test_result and test_result['documents'][0]:
                            logger.info("Vector database seems healthy, skipping reinitialization")
                            return True
                except Exception as e:
                    logger.warning(f"Vector database health check failed: {str(e)}")
                    # Continue with reinitialization
            
            # Use delete_collection and recreate instead of clear_collection
            # This ensures a completely fresh start
            try:
                if self.client is not None:
                    self.client.delete_collection(self.config['collection_name'])
                    logger.info(f"Deleted collection: {self.config['collection_name']}")
            except Exception as e:
                logger.warning(f"Error deleting collection (may not exist): {str(e)}")
                
            # Attempt to reinitialize the ChromaDB client if it failed previously
            if self.client is None:
                try:
                    logger.info("Attempting to reinitialize ChromaDB client")
                    self.client = chromadb.HttpClient(host="localhost", port=8003)
                    logger.info("Successfully reinitialized ChromaDB HTTP client (localhost:8003)")
                except Exception as init_err:
                    logger.error(f"Failed to reinitialize ChromaDB HTTP client: {str(init_err)}")
                    self.use_fallback = True
                    return True
                    
            # Recreate the collection with improved parameters
            try:
                if self.client is not None:
                    self.collection = self._get_or_create_collection()
                    logger.info("Collection recreated with robust parameters")
                else:
                    logger.warning("ChromaDB client is None, cannot recreate collection")
                    self.use_fallback = True
                    return True
            except Exception as create_err:
                logger.error(f"Failed to recreate collection: {str(create_err)}")
                self.use_fallback = True
                return True
                
            # Reinitialize the vectorizer
            self.vectorizer = TfidfVectorizer(
                max_features=self.embedding_dimension,
                ngram_range=(1, 2),
                min_df=1, 
                max_df=0.9
            )
            self.fitted = False
            
            # Create a test document to verify functionality
            test_doc = "Vector database initialization test document"
            test_id = f"init_test_{int(time.time())}"
            test_meta = {"test": "initialization", "timestamp": datetime.now().isoformat()}
            
            # Generate embedding
            test_embedding = self._get_embeddings([test_doc])[0]
            
            # Add test document
            self.collection.upsert(
                ids=[test_id],
                documents=[test_doc],
                embeddings=[test_embedding],
                metadatas=[test_meta]
            )
            
            # Verify it worked
            test_result = self.collection.get(ids=[test_id])
            if test_result and 'documents' in test_result and test_result['documents']:
                logger.info("‚úÖ Vector database successfully reinitialized and verified")
                
                # Clean up test document
                self.collection.delete(ids=[test_id])
                return True
            else:
                logger.error("‚ùå Vector database reinitialization verification failed")
                return False
                
        except Exception as e:
            logger.error(f"Error in clear_and_reinitialize: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def normalize_competitor_data(self):
        """
        Ensure all competitor data in the database has proper metadata fields set.
        This is useful for fixing issues with competitor data after schema changes.
        """
        try:
            # Check if collection exists and has documents
            collection_size = self.get_count()
            if collection_size == 0:
                logger.info("Vector database is empty, no normalization needed")
                return True
                
            logger.info(f"Normalizing competitor data in vector database ({collection_size} documents)")
            
            # Get all documents - ChromaDB doesn't support querying by is_competitor directly in 'where'
            # and also doesn't support 'ids' in include, so we need to get all documents and filter
            try:
                # Get all documents with metadata - no filtering (Chroma limitation)
                results = self.collection.get(include=['metadatas', 'documents'])
                
                if not results or 'metadatas' not in results or not results['metadatas'] or len(results['metadatas']) == 0:
                    logger.info("No documents found for normalization")
                    return False
                    
                # Filter for competitor documents
                competitor_indices = []
                for i, metadata in enumerate(results['metadatas']):
                    if metadata and metadata.get('is_competitor', False):
                        competitor_indices.append(i)
                
                logger.info(f"Found {len(competitor_indices)} competitor documents out of {len(results['metadatas'])} total documents")
                
                # Get document IDs (required for update)
                all_ids = self.collection.get(include=[])['ids']
                competitor_ids = [all_ids[i] for i in competitor_indices]
                competitor_metadatas = [results['metadatas'][i] for i in competitor_indices]
                
                update_count = 0
                for i, metadata in enumerate(competitor_metadatas):
                    needs_update = False
                    
                    # Make sure username and primary_username fields exist
                    if 'username' not in metadata:
                        metadata['username'] = metadata.get('competitor', 'unknown')
                        needs_update = True
                        
                    if 'primary_username' not in metadata:
                        metadata['primary_username'] = metadata.get('associated_primary_account', 'unknown')
                        needs_update = True
                    
                    # Ensure competitor field is set
                    if 'competitor' not in metadata:
                        metadata['competitor'] = metadata['username']
                        needs_update = True
                    
                    # Add enhanced fields for better tracking
                    if 'post_type' not in metadata:
                        metadata['post_type'] = 'competitor'
                        needs_update = True
                    
                    if 'associated_primary_account' not in metadata:
                        metadata['associated_primary_account'] = metadata['primary_username']
                        needs_update = True
                    
                    # Update the document if needed
                    if needs_update:
                        try:
                            # Update just the metadata for this document
                            self.collection.update(
                                ids=[competitor_ids[i]],
                                metadatas=[metadata]
                            )
                            update_count += 1
                        except Exception as update_error:
                            logger.error(f"Error updating competitor document: {str(update_error)}")
                
                logger.info(f"Normalized {update_count}/{len(competitor_indices)} competitor documents")
                return True
                
            except Exception as e:
                logger.error(f"Error retrieving competitor data for normalization: {str(e)}")
                return False
                
        except Exception as e:
            logger.error(f"Error normalizing competitor data: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def _normalize_fallback_usernames(self):
        """
        Normalize usernames in the fallback database.
        """
        try:
            # No fallback database - method no longer needed
            logger.error("Fallback database methods removed - no normalization available")
            return []
        except Exception as e:
            logger.error(f"Error normalizing fallback usernames: {str(e)}")
            return False

    def _check_chroma_server_health(self, host="localhost", port=8003, timeout=2):
        """Check if the ChromaDB HTTP server is up and healthy (v2 API)."""
        try:
            url = f"http://{host}:{port}/api/v2/heartbeat"
            resp = httpx.get(url, timeout=timeout)
            if resp.status_code == 200:
                return True
            else:
                logger.error(f"ChromaDB server health check failed: {resp.status_code} {resp.text}")
                return False
        except Exception as e:
            logger.error(f"ChromaDB server health check exception: {str(e)}")
            return False


def test_vector_db_multi_user():
    """Test vector database operations with primary and secondary usernames."""
    try:
        manager = VectorDatabaseManager()
        manager.clear_collection()  # Start fresh
        
        # Simulate posts from provided files
        primary_username = "maccosmetics"
        secondary_usernames = [
            "anastasiabeverlyhills", "fentybeauty", "narsissist", "toofaced",
            "competitor1", "competitor2", "competitor3", "competitor4", "competitor5"
        ]
        
        sample_posts = [
            # Primary posts
            {"id": "1", "caption": "New lipstick launch!", "engagement": 1200, "likes": 1000, "comments": 200, 
             "timestamp": "2025-04-01T10:00:00Z", "username": primary_username},
            {"id": "2", "caption": "Spring collection reveal", "engagement": 1500, "likes": 1300, "comments": 200, 
             "timestamp": "2025-04-02T12:00:00Z", "username": primary_username},
            # Secondary posts
            {"id": "3", "caption": "Bold brows tutorial", "engagement": 800, "likes": 700, "comments": 100, 
             "timestamp": "2025-04-01T14:00:00Z", "username": "anastasiabeverlyhills"},
            {"id": "4", "caption": "Glowy skin promo", "engagement": 900, "likes": 800, "comments": 100, 
             "timestamp": "2025-04-02T15:00:00Z", "username": "fentybeauty"},
            {"id": "5", "caption": "Matte lip trend", "engagement": 700, "likes": 600, "comments": 100, 
             "timestamp": "2025-04-03T09:00:00Z", "username": "narsissist"}
        ]
        
        # Add posts
        added_count = manager.add_posts(sample_posts, primary_username)
        if added_count != len(sample_posts):
            logger.error(f"Expected {len(sample_posts)} posts, added {added_count}")
            return False
        
        # Test querying
        query = "What's trending in makeup?"
        results_all = manager.query_similar(query, n_results=3)
        results_primary = manager.query_similar(query, n_results=2, filter_username=primary_username)
        results_secondary = manager.query_similar(query, n_results=2, filter_username="fentybeauty")
        
        # Validate results
        if not results_all['documents'][0] or not results_primary['documents'][0] or not results_secondary['documents'][0]:
            logger.error("Query returned empty results")
            return False
        
        logger.info("\nQuery: " + query)
        logger.info("All results:")
        for doc, meta in zip(results_all['documents'][0], results_all['metadatas'][0]):
            logger.info(f"- {doc} (Username: {meta['username']}, Engagement: {meta['engagement']})")
        
        logger.info(f"Primary ({primary_username}) results:")
        for doc, meta in zip(results_primary['documents'][0], results_primary['metadatas'][0]):
            logger.info(f"- {doc} (Engagement: {meta['engagement']})")
        
        logger.info("Secondary (fentybeauty) results:")
        for doc, meta in zip(results_secondary['documents'][0], results_secondary['metadatas'][0]):
            logger.info(f"- {doc} (Engagement: {meta['engagement']})")
        
        if manager.get_count() != added_count:
            logger.error("Collection count mismatch")
            return False
        
        logger.info("Multi-user vector database test successful")
        return True
    except Exception as e:
        logger.error(f"Multi-user test failed: {str(e)}")
        return False


def seed_test_competitor_data():
    """Utility function to seed test competitor data into the vector database."""
    
    # Initialize the vector database
    vdb = VectorDatabaseManager()
    
    # Create some test competitor data
    competitor_data = {
        "fentybeauty": [
            {
                "caption": "New Fenty Beauty collection launching today! #fentybeauty #makeup",
                "engagement": 5000,
                "timestamp": datetime.now().isoformat(),
                "username": "fentybeauty"
            },
            {
                "caption": "Our bestselling foundation now available in 5 new shades! #fentybeauty",
                "engagement": 8003,
                "timestamp": datetime.now().isoformat(),
                "username": "fentybeauty"
            }
        ],
        "toofaced": [
            {
                "caption": "Too Faced new holiday collection is here! #toofaced #makeup #beauty",
                "engagement": 3500,
                "timestamp": datetime.now().isoformat(),
                "username": "toofaced"
            },
            {
                "caption": "Our iconic Better Than Sex mascara has a new look! #toofaced",
                "engagement": 6200,
                "timestamp": datetime.now().isoformat(),
                "username": "toofaced"
            }
        ]
    }
    
    # Add competitor data to the vector database
    total_added = 0
    for competitor, posts in competitor_data.items():
        added = vdb.add_posts(posts, competitor, is_competitor=True)
        total_added += added
        print(f"Added {added} posts for competitor {competitor}")
    
    print(f"Total competitor posts added: {total_added}")
    
    # Verify data was added
    for competitor in competitor_data.keys():
        results = vdb.query_similar("makeup", filter_username=competitor, is_competitor=True)
        found = len(results.get('documents', [[]])[0])
        print(f"Found {found} posts for competitor {competitor}")
    
    return total_added


class FallbackVectorDB:
    """
    Simple fallback vector database using files for storage.
    Used when ChromaDB fails to provide a reliable alternative.
    """
    
    def __init__(self):
        """Initialize the fallback database."""
        self.data_dir = Path("./fallback_vector_db")
        self.data_dir.mkdir(exist_ok=True)
        self.embeddings_file = self.data_dir / "embeddings.json"
        self.documents_file = self.data_dir / "documents.json"
        self.metadata_file = self.data_dir / "metadata.json"
        
        # Load existing data or initialize
        self.embeddings = self._load_json(self.embeddings_file, {})
        self.documents = self._load_json(self.documents_file, {})
        self.metadata = self._load_json(self.metadata_file, {})
        
        logger.info(f"Fallback vector database initialized with {len(self.documents)} documents")
    
    def _load_json(self, file_path, default=None):
        """Load JSON data from file with error handling."""
        try:
            if file_path.exists():
                with open(file_path, 'r') as f:
                    return json.load(f)
            return default if default is not None else {}
        except Exception as e:
            logger.warning(f"Error loading {file_path}: {str(e)}")
            return default if default is not None else {}
    
    def _save_json(self, file_path, data):
        """Save JSON data to file with error handling."""
        try:
            with open(file_path, 'w') as f:
                json.dump(data, f)
            return True
        except Exception as e:
            logger.error(f"Error saving to {file_path}: {str(e)}")
            return False
    
    def add_documents(self, documents, ids, metadatas=None):
        """Add documents to the fallback database."""
        try:
            if not documents or not ids:
                return
                
            if len(documents) != len(ids):
                logger.error("Length mismatch between documents and ids")
                return
                
            # Add each document
            for i, doc_id in enumerate(ids):
                self.documents[doc_id] = documents[i]
                if metadatas and i < len(metadatas):
                    self.metadata[doc_id] = metadatas[i]
                    
            # Save to disk
            self._save_json(self.documents_file, self.documents)
            if metadatas:
                self._save_json(self.metadata_file, self.metadata)
                
            logger.info(f"Added {len(documents)} documents to fallback database")
        except Exception as e:
            logger.error(f"Error adding documents to fallback database: {str(e)}")
    
    def query_similar(self, query_text, n_results=5, filter_username=None, is_competitor=False):
        """
        Basic similarity search using TF-IDF.
        Not as sophisticated as ChromaDB but more reliable.
        """
        try:
            if not self.documents:
                logger.warning("Fallback database is empty")
                return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
                
            # For simplicity, we'll just do a basic text search in the fallback version
            # This isn't as good as proper vector similarity but will work in a pinch
            query_terms = set(query_text.lower().split())
            
            # Score each document based on term overlap
            scores = {}
            for doc_id, text in self.documents.items():
                # Skip if empty
                if not text:
                    continue
                    
                # Check username filter if specified
                if filter_username and doc_id in self.metadata:
                    meta = self.metadata[doc_id]
                    username = meta.get('username', '').lower()
                    primary_username = meta.get('primary_username', '').lower()
                    competitor = meta.get('competitor', '').lower()
                    is_competitor_doc = meta.get('is_competitor', False)
                    
                    # Skip if doesn't match filter criteria
                    if is_competitor and not is_competitor_doc:
                        continue
                    
                    # For competitor content
                    if is_competitor and filter_username.lower() not in [username, primary_username, competitor]:
                        continue
                        
                    # For primary user content
                    if not is_competitor and filter_username.lower() != primary_username.lower():
                        continue
                
                # Calculate simple similarity score
                doc_terms = set(text.lower().split())
                overlap = len(query_terms.intersection(doc_terms))
                
                # Add engagement as a factor if available
                engagement_boost = 1.0
                if doc_id in self.metadata and 'engagement' in self.metadata[doc_id]:
                    engagement = self.metadata[doc_id].get('engagement', 0)
                    if engagement > 0:
                        engagement_boost = min(2.0, 1.0 + (engagement / 10000))
                
                # Calculate final score
                scores[doc_id] = (overlap * engagement_boost) if overlap > 0 else 0
            
            # Sort by score and take top n
            top_ids = sorted(scores.keys(), key=lambda k: scores[k], reverse=True)[:n_results]
            
            # Prepare return values
            documents = [self.documents[doc_id] for doc_id in top_ids if doc_id in self.documents]
            metadatas = [self.metadata[doc_id] for doc_id in top_ids if doc_id in self.metadata]
            distances = [1.0 - (scores[doc_id] / max(1, max(scores.values()))) for doc_id in top_ids]
            
            return {
                'documents': [documents],
                'metadatas': [metadatas],
                'distances': [distances]
            }
        except Exception as e:
            logger.error(f"Error in fallback query: {str(e)}")
            return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
    
    def add_posts(self, posts, primary_username, is_competitor=False):
        """Add posts to the fallback database."""
        try:
            if not posts:
                return 0
                
            # Process posts
            documents = []
            ids = []
            metadatas = []
            
            for post in posts:
                # Get text content
                if 'caption' in post:
                    text = post.get('caption', '').strip()
                elif 'text' in post:
                    text = post.get('text', '').strip()
                elif 'tweet_text' in post:
                    text = post.get('tweet_text', '').strip()
                elif 'full_text' in post:
                    text = post.get('full_text', '').strip()
                else:
                    text = ''  # No text content found
                
                # Get username and timestamp
                username = post.get('username', primary_username)
                if username and username.startswith('@'):
                    username = username[1:]
                    
                timestamp = post.get('timestamp', datetime.now().isoformat())
                
                # Generate ID
                post_id = f"{username}_{abs(hash(text[:100]))}"
                
                # Create metadata
                engagement = post.get('engagement', 0)
                if engagement == 0:
                    likes = post.get('likes', 0) or 0
                    comments = post.get('comments', 0) or 0
                    shares = post.get('shares', 0) or 0
                    retweets = post.get('retweets', 0) or 0
                    replies = post.get('replies', 0) or 0
                    quotes = post.get('quotes', 0) or 0
                    
                    engagement = likes + comments + shares + retweets + replies + quotes
                
                platform = post.get('platform', 'instagram')
                
                metadata = {
                    "username": username,
                    "primary_username": primary_username,
                    "engagement": max(1, engagement),
                    "timestamp": timestamp,
                    "platform": platform,
                    "is_competitor": is_competitor
                }
                
                if is_competitor:
                    metadata["competitor"] = username
                
                # Add to lists
                documents.append(text)
                ids.append(post_id)
                metadatas.append(metadata)
            
            # Add to database
            self.add_documents(documents, ids, metadatas)
            
            return len(documents)
        except Exception as e:
            logger.error(f"Error adding posts to fallback database: {str(e)}")
            return 0

    # ------------------------------------------------------------------
    # Convenience wrapper so callers can quickly add one post without
    # needing to construct the posts list. Zero-post handler uses this.
    # ------------------------------------------------------------------
    def add_post(self, post_id: str, content: str, metadata: dict, is_competitor: bool = False):
        """Add a *single* post to the vector DB.

        Args:
            post_id (str): Unique ID for the post (should already embed
                any disambiguating information like username/timestamp).
            content (str): The textual content of the post.
            metadata (dict): Metadata to attach (username, platform, etc.).
            is_competitor (bool): Flag for duplicate-handling semantics.
        """
        if not content or len(content.strip()) < 3:
            logger.debug("Skipped empty/too-short post in add_post() wrapper")
            return 0

        # Build a mini post dict in the canonical structure expected by
        # add_posts(). We use the 'text' field so duplicate detection
        # downstream works for both Twitter/Instagram.
        post_dict = {
            'id': post_id,
            'text': content,
            'username': metadata.get('username'),
            'timestamp': metadata.get('timestamp', ''),
            'platform': metadata.get('platform', 'instagram'),
        }
        # Merge any additional metadata so fields like `source`,
        # `is_competitor_shadow`, etc., are preserved.
        post_dict.update(metadata or {})

        # Delegate to the batch add method.
        return self.add_posts([post_dict], primary_username=post_dict['username'], is_competitor=is_competitor)

    
    def clear_collection(self):
        """Clear the fallback database."""
        try:
            self.documents = {}
            self.metadata = {}
            self.embeddings = {}
            
            # Save empty files
            self._save_json(self.documents_file, {})
            self._save_json(self.metadata_file, {})
            self._save_json(self.embeddings_file, {})
            
            logger.info("Fallback database cleared")
            return True
        except Exception as e:
            logger.error(f"Error clearing fallback database: {str(e)}")
            return False
    
    def get_count(self):
        """Get the number of documents in the fallback database."""
        return len(self.documents)

    def normalize_usernames(self):
        """Normalize usernames in the fallback database metadata."""
        try:
            if not self.metadata:
                logger.info("No metadata to normalize in fallback database")
                return True
                
            logger.info(f"Normalizing usernames in fallback database ({len(self.metadata)} documents)")
            
            updated_count = 0
            for doc_id, metadata in self.metadata.items():
                if not metadata:
                    continue
                    
                needs_update = False
                
                # Make sure username and primary_username fields exist
                if 'username' not in metadata:
                    metadata['username'] = metadata.get('primary_username', 'unknown')
                    needs_update = True
                    
                if 'primary_username' not in metadata:
                    metadata['primary_username'] = metadata.get('username', 'unknown')
                    needs_update = True
                
                # Remove @ prefix if present in either field
                if metadata['username'] and isinstance(metadata['username'], str) and metadata['username'].startswith('@'):
                    metadata['username'] = metadata['username'][1:]
                    needs_update = True
                    
                if metadata['primary_username'] and isinstance(metadata['primary_username'], str) and metadata['primary_username'].startswith('@'):
                    metadata['primary_username'] = metadata['primary_username'][1:]
                    needs_update = True
                
                # Set is_competitor flag correctly
                if 'is_competitor' not in metadata:
                    metadata['is_competitor'] = False
                    needs_update = True
                
                # Ensure competitor field is set for competitor content
                if metadata.get('is_competitor', False) and 'competitor' not in metadata:
                    metadata['competitor'] = metadata['username']
                    needs_update = True
                
                # Update metadata
                if needs_update:
                    self.metadata[doc_id] = metadata
                    updated_count += 1
            
            # Save updated metadata to disk
            if updated_count > 0:
                self._save_json(self.metadata_file, self.metadata)
                logger.info(f"Updated {updated_count} documents in fallback database")
            
            return True
        except Exception as e:
            logger.error(f"Error normalizing fallback database usernames: {str(e)}")
            return False


if __name__ == "__main__":
    # Run the seed function if this file is executed directly
    seed_test_competitor_data()